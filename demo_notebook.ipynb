{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb7dea4-2986-4069-bc4c-28f3ea13969e",
   "metadata": {},
   "source": [
    "## COVID-19 Findings on Pulmonary X-Ray Images for Classifications with ANN Classifier\n",
    "### Table of Content\n",
    "* [Introduction](#1)\n",
    "* [Libraries](#2)\n",
    "* [Transfer Models](#3)\n",
    "* [Defined Functions](#4)\n",
    "* [Controller](#5)\n",
    "* [Data Preparation](#6)\n",
    "* [Image Generating](#7)\n",
    "* [Transfer Model Preparation](#8)\n",
    "* [Choosing Classifier](#9)\n",
    "* [Result Graphs](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686c1bb-594b-45cc-aef1-3963ea5125ef",
   "metadata": {},
   "source": [
    "<a id = 1></a>\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d79bb3-46e2-414e-a8d4-a3958a12624c",
   "metadata": {},
   "source": [
    "<a id=2></a>\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1956a819-06b5-4741-9f55-41a882e71515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from skimage import exposure\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint,  EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import VGG16, VGG19, InceptionV3, NASNetMobile, NASNetLarge, DenseNet121, ResNet50, Xception, InceptionResNetV2\n",
    "\n",
    "import importlib\n",
    "from skimage import exposure\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1c56c-289c-4fb3-b7ec-549e79fb47aa",
   "metadata": {},
   "source": [
    "<a id=3></a>\n",
    "### Transfer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb991d99-f4aa-4a95-a139-384ce50f6366",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    \n",
    "    models_ = dict(\n",
    "                    VGG16 = dict(\n",
    "                        input_shape = (224,224,3),\n",
    "                        module_name = \"vgg16\",\n",
    "                        last_conv_layer = \"block5_conv3\",\n",
    "                    ),\n",
    "                    VGG19 = dict(\n",
    "                        input_shape = (224,224,3),\n",
    "                        module_name = \"vgg19\",\n",
    "                        last_conv_layer = \"block5_conv4\",\n",
    "                    ),\n",
    "                    # this is used for ChexNet\n",
    "                    DenseNet121=dict(\n",
    "                        input_shape=(224, 224, 3),\n",
    "                        module_name=\"densenet\",\n",
    "                        last_conv_layer=\"conv5_block16_concat\",\n",
    "                    ),\n",
    "                    ResNet50=dict(\n",
    "                        input_shape=(224, 224, 3),\n",
    "                        module_name=\"resnet50\",\n",
    "                        last_conv_layer=\"conv5_block3_out\",\n",
    "                    ),\n",
    "                    InceptionV3=dict(\n",
    "                        input_shape=(299, 299, 3),\n",
    "                        module_name=\"inception_v3\",\n",
    "                        last_conv_layer=\"mixed10\",\n",
    "                    ),\n",
    "                    InceptionResNetV2=dict(\n",
    "                        input_shape=(299, 299, 3),\n",
    "                        module_name=\"inception_resnet_v2\",\n",
    "                        last_conv_layer=\"conv_7b_bn\",\n",
    "                    ),\n",
    "                    NASNetMobile=dict(\n",
    "                        input_shape=(224, 224, 3),\n",
    "                        module_name=\"nasnet\",\n",
    "                        last_conv_layer=\"normal_concat_12\",\n",
    "                    ),\n",
    "                    NASNetLarge=dict(\n",
    "                        input_shape=(331, 331, 3),\n",
    "                        module_name=\"nasnet\",\n",
    "                        last_conv_layer=\"normal_concat_18\",\n",
    "                    ),\n",
    "                    Xception=dict(\n",
    "                        input_shape=(299, 299, 3),\n",
    "                        module_name=\"xception\",\n",
    "                        last_conv_layer=\"block14_sepconv2_act\",\n",
    "                    ),\n",
    "                \n",
    "                )\n",
    "    \n",
    "    return models_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210d6dd-4b90-477c-ac9d-c3a3476de844",
   "metadata": {},
   "source": [
    "<a id = 4></a>\n",
    "### Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00f736a4-b485-4cbd-a1a2-e0338742356d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_last_conv_layer(base_model, model_name):\n",
    "    \n",
    "    models_ = get_models()\n",
    "    layer = base_model.get_layer(models_[model_name][\"last_conv_layer\"])\n",
    "    \n",
    "    return layer\n",
    "\n",
    "# Use this for Covid TL models on Kaggle: https://www.kaggle.com/c/siim-covid19-detection\n",
    "\n",
    "def prepare_data_for_kaggle():\n",
    "    \n",
    "    df_image = pd.read_csv('../input/siim-covid19-detection/train_image_level.csv')\n",
    "    df_study = pd.read_csv('../input/siim-covid19-detection/train_study_level.csv')\n",
    "    df_study['id'] = df_study['id'].str.replace('_study',\"\")\n",
    "    df_study.rename({'id': 'StudyInstanceUID'},axis=1, inplace=True)\n",
    "    df_train = df_image.merge(df_study, on='StudyInstanceUID')\n",
    "    df_train.loc[df_train['Negative for Pneumonia']==1, 'study_label'] = 'negative'\n",
    "    df_train.loc[df_train['Typical Appearance']==1, 'study_label'] = 'typical'\n",
    "    df_train.loc[df_train['Indeterminate Appearance']==1, 'study_label'] = 'indeterminate'\n",
    "    df_train.loc[df_train['Atypical Appearance']==1, 'study_label'] = 'atypical'\n",
    "    df_train.drop(['Negative for Pneumonia','Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance'], axis=1, inplace=True)\n",
    "    df_train['id'] = df_train['id'].str.replace('_image', '.jpg')\n",
    "    df_train['image_label'] = df_train['label'].str.split().apply(lambda x : x[0])\n",
    "    df_size = pd.read_csv('../input/covid-jpg-512/size.csv')\n",
    "    data = df_train.merge(df_size, on='id')\n",
    "    data = data.drop([\"boxes\",\"label\",\"StudyInstanceUID\",\"dim0\",\"dim1\",\"split\"], axis = 1)\n",
    "    img_dir = \"../input/covid-jpg-512/train\"\n",
    "    \n",
    "    return data, img_dir\n",
    "\n",
    "\n",
    "     \n",
    "# If the classifier is ANN, this function plots transfer models' history\n",
    "\n",
    "def plot_tl_metrics(history, model_name):\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist.index += 1\n",
    "        \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(12,12),nrows=2, ncols=1)\n",
    "    hist['loss'].plot(ax=ax1,c='k',label='training loss')\n",
    "    hist['val_loss'].plot(ax=ax1,c='r',linestyle='--', label='validation loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    hist['accuracy'].plot(ax=ax2,c='k',label='training accuracy')\n",
    "    hist['val_accuracy'].plot(ax=ax2,c='r',linestyle='--',label='validation accuracy')\n",
    "    ax2.legend()\n",
    "    plt.suptitle(f\"{model_name} Loss and Accuracy Plots\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Generates images for SVM classifier\n",
    "\n",
    "def prepare_images_for_SVM(train_generator, valid_generator, train_num, val_num):\n",
    "    \n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for i in range(train_num):\n",
    "        x, y = next(train_generator)\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "            \n",
    "    args = (x_list[i] for i in range(train_num))\n",
    "    x_tr = np.vstack((args))\n",
    "    args = (y_list[i] for i in range(train_num))\n",
    "    y_tr = np.vstack(args)\n",
    "    y_tr = y_tr.ravel()\n",
    "            \n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for i in range(val_num):\n",
    "        x, y = next(valid_generator)\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "                \n",
    "    args = (x_list[i] for i in range(val_num))\n",
    "    x_val = np.vstack((args))\n",
    "    args = (y_list[i] for i in range(val_num))\n",
    "    y_val = np.vstack(args)\n",
    "    y_val = y_val.ravel()\n",
    "            \n",
    "    return x_tr, x_val, y_tr, y_val\n",
    "\n",
    "\n",
    "# Extracts features from generated images for SVM classifier \n",
    "    \n",
    "def extract_features_from_images(model, x_tr, x_val, y_tr, y_val):\n",
    "        \n",
    "    x_train = model.predict(x_tr)\n",
    "    x_test = model.predict(x_val)\n",
    "    y_train = y_tr\n",
    "    y_test = y_val\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Prints feature vectors' shapes\n",
    "    \n",
    "def print_feature_shapes(x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    print(\"Extracted data shapes from transfer network\")\n",
    "    print(\"x_train shape: \",x_train.shape)\n",
    "    print(\"x_test shape: \",x_test.shape)\n",
    "    print(\"y_train shape: \",y_train.shape)\n",
    "    print(\"y_test shape: \",y_test.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "# Fits SVM model with Grid Search or Bayes Search Cross Validation   \n",
    "\n",
    "def fit_cross_models(x_train, x_test, y_train, y_test, svm_hyp_search):\n",
    "    \n",
    "    if svm_hyp_search == \"grid\":\n",
    "        \n",
    "        svc_param_grid = {\"kernel\" : [\"rbf\", \"poly\", \"linear\"],\n",
    "                          \"gamma\": [0.001, 0.01, 0.1, 1],\n",
    "                          \"C\": [1,10,50,100,200,300]}\n",
    "        \n",
    "        clf = GridSearchCV(SVC(random_state = 42), param_grid = svc_param_grid, \n",
    "                           cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", \n",
    "                           n_jobs = -1,verbose = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        from skopt import BayesSearchCV\n",
    "        \n",
    "        # log-uniform: understand as search over p = exp(x) by varying x\n",
    "        \n",
    "        search_spaces = {\n",
    "                        'C': (1e-4, 1e+4, 'log-uniform'),\n",
    "                        'gamma': (1e-4, 1e+4, 'log-uniform'),\n",
    "                        'degree': (1, 12),  # integer valued parameter\n",
    "                        'kernel': ['linear', 'poly', 'rbf'],  # categorical parameter\n",
    "                        }\n",
    "        \n",
    "        clf = BayesSearchCV(\n",
    "            SVC(random_state = 42),\n",
    "            search_spaces=search_spaces,\n",
    "            n_iter=32,\n",
    "            cv=3)\n",
    "        \n",
    "        \n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "        \n",
    "    svc = clf.best_estimator_\n",
    "    svc.fit(x_train, y_train)\n",
    "    y_pred = svc.predict(x_test)\n",
    "    \n",
    "    return clf, svc, y_pred\n",
    "\n",
    "\n",
    "# Plots each cross validation split's accuracy scores for number of top pairs of parameters\n",
    "\n",
    "def plot_cv_splits(clf, number_of_top = 7):\n",
    "    \n",
    "    results_df = pd.DataFrame(clf.cv_results_)\n",
    "    results_df = results_df.sort_values(by=['rank_test_score'])\n",
    "    results_df = (\n",
    "        results_df\n",
    "        .set_index(results_df[\"params\"].apply(\n",
    "            lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "        )\n",
    "        .rename_axis('kernel')\n",
    "    )\n",
    "    \n",
    "    # create df of model scores ordered by performance\n",
    "    \n",
    "    model_scores = results_df.filter(regex=r'split\\d*_test_score')\n",
    "    model_scores = model_scores.transpose().iloc[:30,:number_of_top]\n",
    "\n",
    "    # plot 30 examples of dependency between cv fold and AUC scores\n",
    "    \n",
    "    plt.subplots(figsize = (8,12))\n",
    "    sns.lineplot(\n",
    "        data=model_scores,\n",
    "        dashes=False, palette='Set1', marker='o', alpha=.5\n",
    "    )\n",
    "    plt.xlabel(\"CV test fold\")\n",
    "    plt.ylabel(\"Model AUC\")\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Shows each combinations of parameter's accuracy scores on SVM, max_rank = 10 means \"best ten combination\"\n",
    "\n",
    "def plot_cv_scores(clf, max_rank = 10):\n",
    "    \n",
    "    cv_results = pd.DataFrame(clf.cv_results_)\n",
    "    cv_results = cv_results[\n",
    "        ['params', 'rank_test_score', 'mean_test_score', 'std_test_score']\n",
    "    ]\n",
    "    cv_results = cv_results[cv_results['rank_test_score'] < max_rank]\n",
    "    \n",
    "    cv_results = (\n",
    "        cv_results\n",
    "        .set_index(cv_results[\"params\"].apply(\n",
    "            lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "        )\n",
    "        .rename_axis('kernel')\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize = (10,8))\n",
    "    sns.lineplot(data=cv_results['mean_test_score'])\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xlabel(\"Parameters\")\n",
    "    plt.ylabel(\"Mean test score\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Prints best SVM scores and estimator\n",
    "    \n",
    "def print_best_results(clf, svc, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    train_accuracy = svc.score(x_train, y_train)\n",
    "    test_accuracy = svc.score(x_test, y_test)\n",
    "    \n",
    "    print(\"Best SVM estimator (parameters): \", clf.best_estimator_)\n",
    "    print(\"SVM Best Train accuracy\",train_accuracy)\n",
    "    print(\"SVM Best Test Accuracy\",test_accuracy)\n",
    "    \n",
    "    \n",
    "# Shows confusion matrix for ANN classifier\n",
    "        \n",
    "def plot_tl_confusion_matrix(model, valid_generator):\n",
    "    \n",
    "    actual =  valid_generator.labels\n",
    "    preds = np.argmax(model.predict(valid_generator), axis=1)\n",
    "    cfmx = confusion_matrix(actual, preds)\n",
    "    acc = accuracy_score(actual, preds)\n",
    "    \n",
    "    sns.heatmap(cfmx, annot=True, cmap='plasma',\n",
    "        xticklabels=list(valid_generator.class_indices.keys()),\n",
    "            fmt='.0f', \n",
    "            yticklabels=list(valid_generator.class_indices.keys())\n",
    "            )\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Shows confusion matrix for SVM classifier\n",
    "    \n",
    "def plot_svm_confusion_matrix(svc, x_test, y_test):\n",
    "    \n",
    "    actual =  y_test\n",
    "    preds = svc.predict(x_test)\n",
    "    cfmx = confusion_matrix(actual, preds)\n",
    "    acc = accuracy_score(actual, preds)    \n",
    "    \n",
    "    plt.figure()\n",
    "    sns.heatmap(cfmx, annot=True, cmap='plasma',\n",
    "        xticklabels=list(np.unique(y_test)),\n",
    "            fmt='.0f', \n",
    "            yticklabels=list(np.unique(y_test))\n",
    "            )\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Displays imported libraries versions\n",
    "\n",
    "def display_versions(libraries = None):\n",
    "    \n",
    "    from importlib import import_module\n",
    "    \n",
    "    for library in libraries:\n",
    "        print(f\"{library} version: {import_module(library).__version__}\")\n",
    "\n",
    "\n",
    "# Generates image data\n",
    "\n",
    "\n",
    "def generate_images(classifier, classification_type, img_process_function, df_train, df_test, img_dir, img_size, batch_size, validation_split = 0.15):\n",
    "    \n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "    from skimage import exposure\n",
    "    \n",
    "    # Defined image preprocessing functions\n",
    "\n",
    "    def preprocess_function(img):\n",
    "        \n",
    "        if img_process_function == \"equalize_adapthist\":\n",
    "            img = exposure.equalize_adapthist(img/255, clip_limit=0.03, kernel_size=24)\n",
    "        elif img_process_function == \"equalize_hist\":\n",
    "            img = exposure.equalize_hist(img/255, clip_limit=0.03, kernel_size=24)\n",
    "        elif img_process_function == \"rescale_intensity\":\n",
    "            img = exposure.rescale_intensity(img/255, clip_limit=0.03, kernel_size=24)\n",
    "            \n",
    "        return img\n",
    "    \n",
    "    \n",
    "    \n",
    "    # parameters for SVM classifier to be extracted from ImageDataGenerators\n",
    "    \n",
    "    featurewise_center=False\n",
    "    samplewise_center=False\n",
    "    featurewise_std_normalization=False\n",
    "    samplewise_std_normalization=False\n",
    "    zca_whitening=False\n",
    "    rotation_range=0\n",
    "    width_shift_range=0.0\n",
    "    height_shift_range=0.0\n",
    "    brightness_range=None\n",
    "    shear_range=0.0\n",
    "    zoom_range=0.0\n",
    "    channel_shift_range=0.0\n",
    "    cval=0.0\n",
    "    horizontal_flip=False\n",
    "    vertical_flip=False\n",
    "    rescale=None\n",
    "    preprocessing_function=preprocess_function\n",
    "    validation_split=validation_split\n",
    "\n",
    "    \n",
    "    if classification_type == \"binary\":\n",
    "        y_col = \"image_label\"\n",
    "    else:\n",
    "        y_col = \"study_label\"\n",
    "\n",
    "    if classifier == \"ann\":\n",
    "        class_mode = \"categorical\"\n",
    "        zca_epsilon=1e-06\n",
    "        rotation_range=20\n",
    "        width_shift_range=0.0\n",
    "        height_shift_range=0.0\n",
    "        brightness_range=[0.8, 1.1]\n",
    "        shear_range=0.1\n",
    "        zoom_range=0.0\n",
    "        channel_shift_range=0.0\n",
    "        fill_mode='nearest'\n",
    "        cval=0.0\n",
    "        horizontal_flip=False\n",
    "        vertical_flip=False\n",
    "        rescale=None\n",
    "        preprocessing_function=preprocess_function\n",
    "    else:\n",
    "        class_mode = \"raw\"\n",
    "        \n",
    "    \n",
    "    image_generator_train = ImageDataGenerator(\n",
    "                featurewise_center=featurewise_center,\n",
    "                samplewise_center=samplewise_center,\n",
    "                featurewise_std_normalization=featurewise_std_normalization,\n",
    "                samplewise_std_normalization=samplewise_std_normalization,\n",
    "                zca_whitening=zca_whitening,\n",
    "                rotation_range=rotation_range,\n",
    "                width_shift_range=width_shift_range,\n",
    "                height_shift_range=height_shift_range,\n",
    "                brightness_range=brightness_range,\n",
    "                shear_range=shear_range,\n",
    "                zoom_range=zoom_range,\n",
    "                channel_shift_range=channel_shift_range,\n",
    "                cval=cval,\n",
    "                horizontal_flip=horizontal_flip,\n",
    "                vertical_flip=vertical_flip,\n",
    "                rescale=rescale,\n",
    "                preprocessing_function=preprocessing_function,\n",
    "                validation_split=validation_split)\n",
    "        \n",
    "    image_generator_valid = ImageDataGenerator(validation_split=validation_split,\n",
    "                                               preprocessing_function=preprocessing_function)\n",
    "      \n",
    "    \n",
    "    if classifier == \"ann\":\n",
    "        train_generator = image_generator_train.flow_from_dataframe(\n",
    "                    dataframe = df_train,\n",
    "                    directory=img_dir,\n",
    "                    x_col = 'id',\n",
    "                    y_col =  y_col,  \n",
    "                    target_size=(img_size, img_size),\n",
    "                    batch_size=batch_size,\n",
    "                    subset='training', \n",
    "                    seed = 42, \n",
    "                    class_mode = class_mode) \n",
    "            \n",
    "        valid_generator = image_generator_valid.flow_from_dataframe(\n",
    "                dataframe = df_train,\n",
    "                directory=img_dir,\n",
    "                x_col = 'id',\n",
    "                y_col = y_col,\n",
    "                target_size=(img_size, img_size),\n",
    "                batch_size=batch_size,\n",
    "                subset='validation', \n",
    "                shuffle=False,  \n",
    "                seed=42, \n",
    "                class_mode = class_mode)\n",
    "    else:\n",
    "        train_generator = image_generator_train.flow_from_dataframe(\n",
    "                    dataframe = df_train,\n",
    "                    directory=img_dir,\n",
    "                    x_col = 'id',\n",
    "                    y_col =  y_col,  \n",
    "                    target_size=(img_size, img_size),\n",
    "                    batch_size=batch_size,\n",
    "                    seed = 42, \n",
    "                    class_mode = class_mode) \n",
    "            \n",
    "        valid_generator = image_generator_valid.flow_from_dataframe(\n",
    "                dataframe = df_test,\n",
    "                directory=img_dir,\n",
    "                x_col = 'id',\n",
    "                y_col = y_col,\n",
    "                target_size=(img_size, img_size),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,  \n",
    "                seed=42, \n",
    "                class_mode = class_mode)\n",
    "    \n",
    "    return train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15275151-6d61-4b15-846c-5f3fe19bae90",
   "metadata": {},
   "source": [
    "<a id = 5></a>\n",
    "### Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b085f476-aa3e-418d-a1bc-6985843abd5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"DenseNet121\"\n",
    "learning_rate = 0.0001\n",
    "min_learning_rate = 1e-8\n",
    "batch_size = 25\n",
    "epochs = 20\n",
    "verbose = 1\n",
    "img_process_function = \"equalize_adapthist\"\n",
    "isKaggleData = False\n",
    "classification_type = \"multi\"\n",
    "classifier = \"ann\"\n",
    "train_num = 120\n",
    "val_num = 25\n",
    "show_cv_scores = True\n",
    "feature_number = 64\n",
    "use_fine_tuning = True\n",
    "use_chex_weights = False\n",
    "\n",
    "libraries = [\"pandas\",\"numpy\",\"sklearn\",\"tensorflow\",\"keras\",\"skimage\",\"matplotlib\",\"seaborn\"]\n",
    "show_versions = True\n",
    "svm_hyp_search = \"grid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326b0a8-6743-4619-8565-82adac9e09b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id = 6></a>\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d27774eb-bf86-4d18-ac00-eb3c2f123729",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.2.4\n",
      "numpy version: 1.20.1\n",
      "sklearn version: 0.24.1\n",
      "tensorflow version: 2.3.0\n",
      "keras version: 2.4.3\n",
      "skimage version: 0.18.1\n",
      "matplotlib version: 3.3.4\n",
      "seaborn version: 0.11.1\n"
     ]
    }
   ],
   "source": [
    "if show_versions:\n",
    "    display_versions(libraries)\n",
    "\n",
    "models_ = get_models()\n",
    "input_shape = models_[model_name][\"input_shape\"]\n",
    "img_size = input_shape[0]\n",
    "\n",
    "\n",
    "if isKaggleData:\n",
    "    data, img_dir = prepare_data_for_kaggle()\n",
    "else:\n",
    "    data = pd.read_csv(\"train_data.csv\")\n",
    "    img_dir = \"images/train\"\n",
    "    \n",
    "df_data = data.copy()\n",
    "\n",
    "# drop images from dataframe not in images directory\n",
    "files = os.listdir(\"images/train\")\n",
    "\n",
    "not_in_files_index = []\n",
    "\n",
    "for file_id in df_data.id:\n",
    "    if file_id in files:\n",
    "        continue\n",
    "    else:\n",
    "        not_in_files_index.append(df_data[df_data[\"id\"] == file_id].index[0])\n",
    "        \n",
    "df_data = df_data.drop(not_in_files_index, axis = 0)\n",
    "\n",
    "# drop images that have unclear view\n",
    "drop_df = pd.read_excel(\"dropped_image_IDs.xlsx\") + \".jpg\"\n",
    "\n",
    "drop_index = []\n",
    "for row in drop_df.values:\n",
    "    drop_index.append(df_data[df_data[\"id\"] == row[0]].index[0])\n",
    "    \n",
    "    \n",
    "df_data = df_data.drop(drop_index, axis = 0)\n",
    "\n",
    "# splitting images train and test\n",
    "df_train = df_data.iloc[:5000]\n",
    "df_test = df_data.iloc[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700e5e4-fce9-48c4-8741-10c3b3b8ead8",
   "metadata": {},
   "source": [
    "<a id = 7></a>\n",
    "### Image Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb0f3551-9924-4803-8023-88a292c3e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4250 validated image filenames belonging to 4 classes.\n",
      "Found 750 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, valid_generator = generate_images( classifier = classifier, \n",
    "                                                   classification_type = classification_type, \n",
    "                                                   img_process_function = classification_type, \n",
    "                                                   df_train = df_train, \n",
    "                                                   df_test = df_test, \n",
    "                                                   img_dir = img_dir, \n",
    "                                                   img_size = img_size, \n",
    "                                                   batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58d31225-a16b-45cc-bed6-bef48110a66a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2485\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2486\u001b[1;33m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2487\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Operation 'gradient_tape/functional_1/conv3_block1_2_conv/Conv2D/Conv2DBackpropInput' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps_utils.py\u001b[0m in \u001b[0;36mget_read_write_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mread_only_input_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAD_ONLY_RESOURCE_INPUTS_ATTR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2489\u001b[0m       \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2490\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2491\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Operation 'gradient_tape/functional_1/conv3_block1_2_conv/Conv2D/Conv2DBackpropInput' has no attr named '_read_only_resource_inputs'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-0508e771106e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# Model fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         history = model.fit(\n\u001b[0m\u001b[0;32m     60\u001b[0m               \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         if x is not None)\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m     \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    396\u001b[0m       \u001b[1;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m       \u001b[1;31m# and last_write_to_resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m   \u001b[1;34m\"\"\"Returns an iterable of resources touched by this `op`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m   \u001b[0mreads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrites\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_read_write_resource_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m   \u001b[0msaturated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m   \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msaturated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps_utils.py\u001b[0m in \u001b[0;36mget_read_write_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    110\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# Attr was not set. Add all resource inputs to `writes` and return.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mwrites\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrites\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minputs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs_val\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m       self._inputs_val = tuple(\n\u001b[0m\u001b[0;32m   2318\u001b[0m           map(self.graph._get_tensor_by_tf_output,\n\u001b[0;32m   2319\u001b[0m               pywrap_tf_session.GetOperationInputs(self._c_op)))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_tensor_by_tf_output\u001b[1;34m(self, tf_output)\u001b[0m\n\u001b[0;32m   3859\u001b[0m       \u001b[0mThe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mrepresents\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3860\u001b[0m     \"\"\"\n\u001b[1;32m-> 3861\u001b[1;33m     \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_operation_by_tf_operation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3862\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_operation_by_tf_operation\u001b[1;34m(self, tf_oper)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3823\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_operation_by_tf_operation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_oper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3824\u001b[1;33m     \u001b[0mop_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_oper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3825\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_operation_by_name_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% Model call, training and evaluating\n",
    "\n",
    "base_model_class = getattr(\n",
    "    importlib.import_module(\n",
    "        f\"keras.applications.{models_[model_name]['module_name']}\"\n",
    "        ),\n",
    "        model_name)\n",
    "          \n",
    "img_input = Input(shape = input_shape)\n",
    "        \n",
    "base_model = base_model_class(\n",
    "            include_top = False,\n",
    "            input_tensor = img_input,\n",
    "            input_shape = input_shape,\n",
    "            weights = \"imagenet\",\n",
    "            pooling = \"avg\")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "if (model_name == \"DenseNet121\") & use_chex_weights:\n",
    "    \n",
    "    chex_weights_path = '../input/chexnet-weights/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\n",
    "    out = Dense(14, activation='sigmoid')(base_model.output)\n",
    "    base_model = Model(inputs=base_model.input, outputs=out)\n",
    "    base_model.load_weights(chex_weights_path)\n",
    "    x = get_last_conv_layer(base_model, model_name).output\n",
    "    output = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "else:\n",
    "    x = get_last_conv_layer(base_model, model_name).output\n",
    "    output = GlobalAveragePooling2D()(x)\n",
    "\n",
    "if use_fine_tuning:   \n",
    "    base_model.trainable = True\n",
    "    \n",
    "    if classification_type == \"multi\":\n",
    "        predictions = Dense(len(df_train.study_label.unique()), activation = \"softmax\", name = \"multi_predictions\")(output)\n",
    "        model = Model(base_model.input, predictions)\n",
    "        model.compile(Adam(lr=learning_rate),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "    else:\n",
    "        predictions = Dense(len(df_train.image_label.unique()), activation = \"softmax\", name = \"binary_predictions\")(output)\n",
    "        model = Model(base_model.input, predictions)\n",
    "        model.compile(Adam(lr=learning_rate),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "    if classifier == \"ann\":\n",
    "        # Keras callbacks\n",
    "        rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, verbose = verbose, \n",
    "                                    min_delta = 1e-4, min_lr = min_learning_rate, mode = 'min')\n",
    "    \n",
    "        es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 5, mode = 'min', \n",
    "                        restore_best_weights = True, verbose = verbose)\n",
    "    \n",
    "        ckp = ModelCheckpoint('model.h5',monitor = 'val_loss',\n",
    "                          verbose = verbose, save_best_only = True, mode = 'min')\n",
    "    \n",
    "        # Model fitting\n",
    "        history = model.fit(\n",
    "              train_generator,\n",
    "              epochs= epochs,\n",
    "              validation_data=valid_generator,\n",
    "              callbacks=[es, rlr, ckp],\n",
    "              verbose= verbose\n",
    "              )\n",
    "    else:\n",
    "        # Model fitting\n",
    "        history = model.fit(\n",
    "              train_generator,\n",
    "              epochs= epochs,\n",
    "              validation_data=valid_generator,\n",
    "              callbacks=[es, rlr, ckp],\n",
    "              verbose= verbose\n",
    "              )\n",
    "        \n",
    "    \n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c12ef0-26d0-4985-8c85-6b8dc650344c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
