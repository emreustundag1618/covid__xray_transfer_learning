{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29e8b7c-566b-445a-868c-a739ef6c4efb",
   "metadata": {},
   "source": [
    "## SIIM Data Covid-19 ANN classifier\n",
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e20bdfd-45cc-4db2-a879-d5cecd2a97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"DenseNet121\"\n",
    "learning_rate = 0.0001\n",
    "min_learning_rate = 1e-8\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "verbose = 1\n",
    "img_process_function = \"equalize_adapthist\"\n",
    "isKaggleData = False\n",
    "classification_type = \"multi\"\n",
    "classifier = \"ann\"\n",
    "\n",
    "use_fine_tuning = True\n",
    "use_chex_weights = True\n",
    "\n",
    "libraries = [\"pandas\",\"numpy\",\"sklearn\",\"tensorflow\",\"keras\",\"skimage\",\"matplotlib\",\"seaborn\"]\n",
    "show_versions = False\n",
    "svm_hyp_search = \"grid\"\n",
    "show_model_summary = False\n",
    "save_weights = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd3c78-de78-4e9b-a337-0c8544aa8dfd",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d2c643-c8c1-44df-a1a4-337578a48fe1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from skimage import exposure\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint,  EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.applications import VGG16, VGG19, InceptionV3, NASNetMobile, NASNetLarge, DenseNet121, ResNet50, Xception, InceptionResNetV2\n",
    "\n",
    "import importlib\n",
    "from skimage import exposure\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71f28b36-2779-4997-8872-7327c7d6c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_versions(libraries = None):\n",
    "    \n",
    "    from importlib import import_module\n",
    "    \n",
    "    for library in libraries:\n",
    "        print(f\"{library} version: {import_module(library).__version__}\")\n",
    "\n",
    "if show_versions:\n",
    "    display_versions(libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a60705-ac39-40ea-af73-1b86b8432c61",
   "metadata": {},
   "source": [
    "#### Transfer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b2a534-ee7b-4021-a3b4-5de5c4faf275",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16, VGG19, InceptionV3, NASNetMobile, NASNetLarge, DenseNet121, ResNet50, Xception, InceptionResNetV2\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    \n",
    "    models_ = dict(\n",
    "                    VGG16 = dict(\n",
    "                        input_shape = (224,224,3),\n",
    "                        module_name = \"vgg16\",\n",
    "                        last_conv_layer = \"block5_conv3\",\n",
    "                    ),\n",
    "                    VGG19 = dict(\n",
    "                        input_shape = (224,224,3),\n",
    "                        module_name = \"vgg19\",\n",
    "                        last_conv_layer = \"block5_conv4\",\n",
    "                    ),\n",
    "                    # this is used for ChexNet\n",
    "                    DenseNet121=dict(\n",
    "                        input_shape=(224, 224, 3),\n",
    "                        module_name=\"densenet\",\n",
    "                        last_conv_layer=\"conv5_block16_concat\",\n",
    "                    ),\n",
    "                    ResNet50=dict(\n",
    "                        input_shape=(224, 224, 3),\n",
    "                        module_name=\"resnet50\",\n",
    "                        last_conv_layer=\"conv5_block3_out\",\n",
    "                    ),\n",
    "                    InceptionV3=dict(\n",
    "                        input_shape=(299, 299, 3),\n",
    "                        module_name=\"inception_v3\",\n",
    "                        last_conv_layer=\"mixed10\",\n",
    "                    ),\n",
    "                    InceptionResNetV2=dict(\n",
    "                        input_shape=(299, 299, 3),\n",
    "                        module_name=\"inception_resnet_v2\",\n",
    "                        last_conv_layer=\"conv_7b_bn\",\n",
    "                    ),\n",
    "                    NASNetMobile=dict(\n",
    "                        input_shape=(224, 224, 3),\n",
    "                        module_name=\"nasnet\",\n",
    "                        last_conv_layer=\"normal_concat_12\",\n",
    "                    ),\n",
    "                    NASNetLarge=dict(\n",
    "                        input_shape=(331, 331, 3),\n",
    "                        module_name=\"nasnet\",\n",
    "                        last_conv_layer=\"normal_concat_18\",\n",
    "                    ),\n",
    "                    Xception=dict(\n",
    "                        input_shape=(299, 299, 3),\n",
    "                        module_name=\"xception\",\n",
    "                        last_conv_layer=\"block14_sepconv2_act\",\n",
    "                    ),\n",
    "                \n",
    "                )\n",
    "    \n",
    "    return models_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d369304f-1a25-4434-a051-fdbd66c1f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ = get_models()\n",
    "input_shape = models_[model_name][\"input_shape\"]\n",
    "img_size = input_shape[0]\n",
    "\n",
    "\n",
    "if isKaggleData:\n",
    "    data, img_dir = prepare_data_for_kaggle()\n",
    "else:\n",
    "    data = pd.read_csv(\"../train_data.csv\")\n",
    "    img_dir = \"../images/train\"\n",
    "    \n",
    "df_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fc18e-f1f6-44ed-b982-6858364e2e27",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a78dfeb-1a44-402f-8734-15ffbe695697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop images from dataframe not in images directory\n",
    "files = os.listdir(\"../images/train\")\n",
    "\n",
    "not_in_files_index = []\n",
    "\n",
    "for file_id in df_data.id:\n",
    "    if file_id in files:\n",
    "        continue\n",
    "    else:\n",
    "        not_in_files_index.append(df_data[df_data[\"id\"] == file_id].index[0])\n",
    "        \n",
    "df_data = df_data.drop(not_in_files_index, axis = 0)\n",
    "\n",
    "# drop images that have unclear view\n",
    "drop_df = pd.read_excel(\"../dropped_image_IDs.xlsx\") + \".jpg\"\n",
    "\n",
    "drop_index = []\n",
    "for row in drop_df.values:\n",
    "    drop_index.append(df_data[df_data[\"id\"] == row[0]].index[0])\n",
    "    \n",
    "    \n",
    "df_data = df_data.drop(drop_index, axis = 0)\n",
    "\n",
    "# splitting images train and test\n",
    "df_train = df_data.iloc[:5000]\n",
    "df_test = df_data.iloc[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6a7ee-bfd2-4f46-8927-d7fe72261c8f",
   "metadata": {},
   "source": [
    "#### Image Generators for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fd820f8-0e59-4d70-9b03-143fc2064e78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4250 validated image filenames belonging to 4 classes.\n",
      "Found 750 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "def generate_images_for_model_training(classifier, classification_type, img_process_function, df_train, df_test, img_dir, img_size, batch_size, validation_split = 0.15):\n",
    "    \n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "    from skimage import exposure\n",
    "    \n",
    "    # Defined image preprocessing functions\n",
    "\n",
    "    def preprocess_function(img):\n",
    "        \n",
    "        if img_process_function == \"equalize_adapthist\":\n",
    "            img = exposure.equalize_adapthist(img/255, clip_limit=0.03, kernel_size=24)\n",
    "        elif img_process_function == \"equalize_hist\":\n",
    "            img = exposure.equalize_hist(img/255, clip_limit=0.03, kernel_size=24)\n",
    "        elif img_process_function == \"rescale_intensity\":\n",
    "            img = exposure.rescale_intensity(img/255, clip_limit=0.03, kernel_size=24)\n",
    "            \n",
    "        return img\n",
    "    \n",
    "    if classification_type == \"binary\":\n",
    "        y_col = \"image_label\"\n",
    "    else:\n",
    "        y_col = \"study_label\"\n",
    "    \n",
    "    \n",
    "    image_generator_train = ImageDataGenerator(\n",
    "                    featurewise_center=False,\n",
    "                    samplewise_center=False,\n",
    "                    featurewise_std_normalization=False,\n",
    "                    samplewise_std_normalization=False,\n",
    "                    zca_epsilon=1e-06,\n",
    "                    zca_whitening=False,\n",
    "                    width_shift_range=0.0,\n",
    "                    height_shift_range=0.0,\n",
    "                    brightness_range=[0.8, 1.1],\n",
    "                    shear_range=0.1,\n",
    "                    zoom_range=0.0,\n",
    "                    channel_shift_range=0.0,\n",
    "                    cval=0.0,\n",
    "                    horizontal_flip=False,\n",
    "                    vertical_flip=False,\n",
    "                    rescale=None,\n",
    "                    rotation_range=20,\n",
    "                    preprocessing_function=preprocess_function,\n",
    "                    validation_split=validation_split)\n",
    "        \n",
    "    image_generator_valid = ImageDataGenerator(validation_split=validation_split,\n",
    "                                               preprocessing_function=preprocess_function)\n",
    "      \n",
    "\n",
    "    train_generator = image_generator_train.flow_from_dataframe(\n",
    "                dataframe = df_train,\n",
    "                directory=img_dir,\n",
    "                x_col = 'id',\n",
    "                y_col =  y_col,  \n",
    "                target_size=(img_size, img_size),\n",
    "                batch_size=batch_size,\n",
    "                subset='training', \n",
    "                seed = 42, \n",
    "                class_mode = \"categorical\") \n",
    "        \n",
    "    valid_generator = image_generator_valid.flow_from_dataframe(\n",
    "            dataframe = df_train,\n",
    "            directory=img_dir,\n",
    "            x_col = 'id',\n",
    "            y_col = y_col,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            subset='validation', \n",
    "            shuffle=False,  \n",
    "            seed=42, \n",
    "            class_mode = \"categorical\")\n",
    "    \n",
    "    return train_generator, valid_generator\n",
    "\n",
    "train_generator, valid_generator = generate_images_for_model_training( classifier = classifier, \n",
    "                                                                       classification_type = classification_type, \n",
    "                                                                       img_process_function = img_process_function, \n",
    "                                                                       df_train = df_train, \n",
    "                                                                       df_test = df_test, \n",
    "                                                                       img_dir = img_dir, \n",
    "                                                                       img_size = img_size, \n",
    "                                                                       batch_size = batch_size, \n",
    "                                                                       validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2889b6-8179-48d7-b50a-a3bbc3000fd3",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff991809-4bb3-43fe-8c6d-eeb73c54420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_conv_layer(base_model, model_name):\n",
    "    \n",
    "    models_ = get_models()\n",
    "    layer = base_model.get_layer(models_[model_name][\"last_conv_layer\"])\n",
    "    \n",
    "    return layer\n",
    "\n",
    "base_model_class = getattr(\n",
    "    importlib.import_module(\n",
    "        f\"keras.applications.{models_[model_name]['module_name']}\"\n",
    "        ),\n",
    "        model_name)\n",
    "          \n",
    "img_input = Input(shape = input_shape)\n",
    "        \n",
    "base_model = base_model_class(\n",
    "            include_top = False,\n",
    "            input_tensor = img_input,\n",
    "            input_shape = input_shape,\n",
    "            weights = \"imagenet\",\n",
    "            pooling = \"avg\")\n",
    "\n",
    "if (model_name == \"DenseNet121\") & use_chex_weights:\n",
    "    \n",
    "    chex_weights_path = 'brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\n",
    "    out = Dense(14, activation='sigmoid')(base_model.output)\n",
    "    base_model = Model(inputs=base_model.input, outputs=out)\n",
    "    base_model.load_weights(chex_weights_path)\n",
    "    x = get_last_conv_layer(base_model, model_name).output\n",
    "    output = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "else:\n",
    "    x = get_last_conv_layer(base_model, model_name).output\n",
    "    output = GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd6c4d-b805-4698-b91c-38ee0d122c19",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5514a805-267f-46cd-8780-2e1b3b98cfb6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/133 [..............................] - ETA: 19:40 - loss: 1.3145 - accuracy: 0.3281"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-cce94b0e7fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Model fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     history = model.fit(\n\u001b[0m\u001b[0;32m     31\u001b[0m           \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "if use_fine_tuning:   \n",
    "    base_model.trainable = True\n",
    "    \n",
    "    if classification_type == \"multi\":\n",
    "        predictions = Dense(len(df_train.study_label.unique()), activation = \"softmax\", name = \"multi_predictions\")(output)\n",
    "        model = Model(base_model.input, predictions)\n",
    "        model.compile(Adam(lr=learning_rate),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "    else:\n",
    "        predictions = Dense(len(df_train.image_label.unique()), activation = \"softmax\", name = \"binary_predictions\")(output)\n",
    "        model = Model(base_model.input, predictions)\n",
    "        model.compile(Adam(lr=learning_rate),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "    if show_model_summary:\n",
    "        print(model.summary())\n",
    "        \n",
    "    # Keras callbacks\n",
    "    rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, verbose = verbose, \n",
    "                                    min_delta = 1e-4, min_lr = min_learning_rate, mode = 'min')\n",
    "    \n",
    "    es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 5, mode = 'min', \n",
    "                        restore_best_weights = True, verbose = verbose)\n",
    "    \n",
    "    ckp = ModelCheckpoint('model.h5',monitor = 'val_loss',\n",
    "                          verbose = verbose, save_best_only = True, mode = 'min')\n",
    "    \n",
    "    # Model fitting\n",
    "    history = model.fit(\n",
    "          train_generator,\n",
    "          epochs= epochs,\n",
    "          validation_data=valid_generator,\n",
    "          callbacks=[es, rlr, ckp],\n",
    "          verbose= verbose\n",
    "          )\n",
    "    \n",
    "    if save_weights:\n",
    "        model.save_weights(f\"{model_name}-model.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d51fa4-ad72-45a8-bb79-71ab324825d0",
   "metadata": {},
   "source": [
    "#### Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b448534-020f-43b4-b7df-1baa2a4a9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tl_metrics(history, model_name):\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist.index += 1\n",
    "        \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(12,12),nrows=2, ncols=1)\n",
    "    hist['loss'].plot(ax=ax1,c='k',label='training loss')\n",
    "    hist['val_loss'].plot(ax=ax1,c='r',linestyle='--', label='validation loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    hist['accuracy'].plot(ax=ax2,c='k',label='training accuracy')\n",
    "    hist['val_accuracy'].plot(ax=ax2,c='r',linestyle='--',label='validation accuracy')\n",
    "    ax2.legend()\n",
    "    plt.suptitle(f\"{model_name} Loss and Accuracy Plots\")\n",
    "    plt.show()\n",
    "\n",
    "# plot_tl_metrics(history, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32894dbd-b78d-4310-9b85-3aa3d0db8ab6",
   "metadata": {},
   "source": [
    "#### Generate Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca3949f0-f8fa-4c9c-b018-dbd62b178a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 946 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "def generate_test_images(classifier, classification_type, \n",
    "                       img_process_function, df_train, df_test, img_dir, \n",
    "                       img_size, batch_size, validation_split = 0.15):\n",
    "\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "    from skimage import exposure\n",
    "    \n",
    "    \n",
    "    if classification_type == \"binary\":\n",
    "        y_col = \"image_label\"\n",
    "    else:\n",
    "        y_col = \"study_label\"\n",
    "\n",
    "    image_generator_test = ImageDataGenerator()\n",
    "    \n",
    "    test_generator = image_generator_test.flow_from_dataframe(\n",
    "            dataframe = df_test,\n",
    "            directory=img_dir,\n",
    "            x_col = 'id',\n",
    "            y_col = y_col,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,  \n",
    "            seed=42, \n",
    "            class_mode = \"categorical\")\n",
    "    \n",
    "    return test_generator\n",
    "\n",
    "test_generator = generate_test_images(classifier, \n",
    "                                     classification_type, \n",
    "                                     img_process_function, \n",
    "                                     df_train, df_test, \n",
    "                                     img_dir, img_size, batch_size, validation_split = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0569c-7ed1-491d-bc08-48454c9e0063",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3662d9fa-5824-447b-8ed1-3382f6d733a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_tl_confusion_matrix(model, test_generator):\n",
    "    \n",
    "    actual =  test_generator.labels\n",
    "    preds = np.argmax(model.predict(test_generator), axis=1)\n",
    "    cfmx = confusion_matrix(actual, preds)\n",
    "    acc = accuracy_score(actual, preds)\n",
    "    \n",
    "    sns.heatmap(cfmx, annot=True, cmap='plasma',\n",
    "        xticklabels=list(test_generator.class_indices.keys()),\n",
    "            fmt='.0f', \n",
    "            yticklabels=list(test_generator.class_indices.keys())\n",
    "            )\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()\n",
    "\n",
    "plot_tl_confusion_matrix(model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76909b-bbcc-48ee-b531-90d0f6584acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
